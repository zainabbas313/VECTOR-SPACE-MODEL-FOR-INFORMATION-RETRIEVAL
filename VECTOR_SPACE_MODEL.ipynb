{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f40d2ae7-c9eb-41f1-bb16-eda4319547cc",
   "metadata": {},
   "source": [
    "# Research Papers Text Analysiss.\r\n",
    "\r\n",
    "## Table of Contents\r\n",
    "\r\n",
    "- [Requirements](#requirements)\r\n",
    "- [Functions](#functions)\r\n",
    "\r\n",
    "## Requirements\r\n",
    "\r\n",
    "This script requires Python 3.x along with the following libraries:\r\n",
    "\r\n",
    "- nltk\r\n",
    "- os\r\n",
    "- string\r\n",
    "- nltk.corpus.words\r\n",
    "- nltk.tokenize.word_tokenize\r\n",
    "- nltk.stem.porter.PorterStemmer\r\n",
    "- nltk.corpus.wordnet\r\n",
    "- math\r\n",
    "\r\n",
    "## Functions\r\n",
    "\r\n",
    "Several functions are defined in the script to handle different aspects of text analysis:\r\n",
    "\r\n",
    "- `extract_data(directory)`: Extracts data from text files in the specified directory, including tokenization and filtering based on various criteria.\r\n",
    "- `cleaning_pipeline(token)`: Cleans and processes tokens, removing unnecessary characters, splitting compound words, and filtering out non-English words.\r\n",
    "- `token_seperator(token)`: Separates tokens into individual words using a word segmentation algorithm.\r\n",
    "- `remove_conjunctions_from_sets(input_list)`: Removes conjunctions from a list of tuples containing word sets and their associated information.\r\n",
    "- `word_segment(sentences)`: Segments sentences into individual words and filters out stop words.\r\n",
    "- `build_inverted_index(tokens)`: Builds an inverted index for efficient search operations based on the tokens.\r\n",
    " an example for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0ac2d46c-0591-481a-8ac8-bba3c7dc6c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "from math import log\n",
    "import csv\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "200625ae-90a7-4e2f-87af-048c6d57d64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Zain\n",
      "[nltk_data]     Abbas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to C:\\Users\\Zain\n",
      "[nltk_data]     Abbas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95283c8-dbcb-4371-bf78-e541b6d3e8f8",
   "metadata": {},
   "source": [
    "# COMMENTs\n",
    "\n",
    "## COMMENT 1\n",
    "\n",
    "### Purpose\n",
    "\n",
    "The main function of this module is to categorize tokens into different lists based on specific conditions. Two types of variables are created: boolean index and positional index. Boolean index, also known as an inverted index, is denoted by variables prefixed with 'boolean_', making it easy to identify throughout the code. Similarly, positional index variables store tokens along with their positions.\n",
    "\n",
    "### Function Naming Convention\n",
    "\n",
    "All function names have the suffix:\n",
    "   - `boolean` for boolean index.\n",
    "   - `positional` for positional index.\n",
    "\n",
    "### VARIBALES\n",
    "\n",
    "- **Boolean Token 1**: Tokens with a length between 3 and 14 characters, without punctuation, and not consisting entirely of digits.\n",
    "- **Positional Token 1**: Same as boolean token 1, but tokens are also stored with their positions.\n",
    "- **Boolean Token of Length 2**: Tokens exactly 2 characters long and not consisting entirely of digits.\n",
    "- **Positional Token of Length 2**: Same as boolean token of length 2, but tokens are also stored with their positions.\n",
    "- **Boolean Token as Sentence**: Tokens longer than 14 characters, without hyphens, and not consisting entirely of digits.\n",
    "- **Positional Token as Sentence**: Same as boolean token as sentence, but tokens are also stored with their positions.\n",
    "- **Boolean Token with Hyphen**: Tokens containing hyphens.\n",
    "- **Positional Token with Hyphen**: Same as boolean token with hyphen, but tokens are also stored with their positions.\n",
    "- **Boolean Token with Punctuation**: Tokens containing punctuation (excluding hyphens), with a length less than or equal to 14 characters, and not consisting entirely of digits.\n",
    "- **Positional Token with Punctuation**: Same as boolean token with punctuation, but tokens are also stored with their positions.\n",
    "- **Positional All Number**: Tokens consisting entirely of digits, stored along with their positions.\n",
    "- **Boolean All Number**: Tokens consisting entirely of digits.\n",
    "- **Boolean All Token**: Tokens not meeting any specific condition.\n",
    "\n",
    "\n",
    "## COMMENT 2\n",
    "\n",
    "### Cleaning Pipeline for Boolean and Positional Indexing\r\n",
    "\r\n",
    "The `cleaning_pipeline` function performs token cleaning and preprocessing for both boolean and positional indexing purposes. Here's a concise breakdown of its functionality:\r\n",
    "#\r\n",
    "## Purose\r\n",
    "\r\n",
    "This function is designed to clean and preprocess tokens for indexing purposes, distinguishing between boolean and positional indexing need#s.\r\n",
    "\r\n",
    "## Functionality\r\n",
    "\r\n",
    "1. **Initialization**: \r\n",
    "   - Initialize lists to store different types of tokens for both boolean and positional indexing.\r\n",
    "\r\n",
    "2. **Token Processing Loop**:\r\n",
    "   - Iterate through each token provided.\r\n",
    "   - Clean tokens by replacing punctuation with spaces and splitting if they contain spaces.\r\n",
    "   - Separate tokens consisting entirely of digits.\r\n",
    "\r\n",
    "3. **Post-Processing Loop**:\r\n",
    "   - For each cleaned token:\r\n",
    "     - Remove digits if present.\r\n",
    "     - Split into words and filter based on length.\r\n",
    "     - Filter tokens based on English word list.\r\n",
    "\r\n",
    "4. **Return Values**:\r\n",
    "   - Return three lists for both boolean and positional indexing:\r\n",
    "     - Tokens not needing further processing.\r\n",
    "     - Tokens needing further processing.\r\n",
    "     - Tokens cons`p\n",
    "\n",
    "# COMMENT 3\n",
    "\n",
    "I integrate the concept of Zipf's Law into the explanation of the code by emphasizing how it influences word frequency calculations and token extraction. \n",
    "\n",
    "### EXPLANATION\r\n",
    "\r\n",
    "1. **token_seperator(token)**:\r\n",
    "   - This function takes a list of tokens as input.\r\n",
    "   - It initializes an empty list called `words`.\r\n",
    "   - It iterates over each word in `english_words` and appends it to the `words` list. This assumes that `english_words` is a pre-defined list of English words.\r\n",
    "   - It then iterates over pairs of boolean tokens represented by `boolean_token1`, where each pair `(x, y)` seems to represent a boolean token and its corresponding value.\r\n",
    "   - Each word from these tokens is appended to the `words` list.\r\n",
    "   - It calculates the cost of each word using a formula based on its frequency and position in the list of words, potentially leveraging Zipf's Law to estimate word frequencies.\r\n",
    "   - It finds the maximum length of words in the `words` list.\r\n",
    "   - It calls the `infer_spaces_boolean` function for each token in the input, converts it to lowercase, and computes its clean version along with the original token's corresponding boolean value.\r\n",
    "   - Finally, it returns the result of the `cleaning_pipeline_boolean` function applied to the cleaned terms.\r\n",
    "\r\n",
    "2. **infer_spaces(s, maxword, wordcost)**:\r\n",
    "   - This function takes a string `s`, maximum word length `maxword`, and a dictionary `wordcost` as input.\r\n",
    "   - It defines a nested function `best_match(i)` that finds the best matching words for a given index `i` in the string.\r\n",
    "   - It initializes a list `cost` with the cost of starting from each character as 0.\r\n",
    "   - It iterates over each character in the input string `s`.\r\n",
    "   - For each character, it calculates the cost of matching the substring ending at that character with the known words and updates the `cost` list accordingly, potentially taking into account Zipf's Law in estimating word frequencies.\r\n",
    "   - It constructs the best matching words by backtracking from the end of the string using the `best_match(i)` function.\r\n",
    "   - Finally, it joins the reversed list of best matching words with spaces and returns the result.\r\n",
    "\r\n",
    "By incorporating the idea of Zipf's Law Ito the explanation, we highlight how word frequencies, and consequently word costs, are calculated, adding depth to the understanding of the \n",
    "\n",
    "# COMMENT 4\n",
    "\n",
    "Two functions appear to be part of a text processing pipeline, likely aimed at cleaning and segmenting text data.\n",
    "\n",
    "### EXPLANATION\r\n",
    "\r\n",
    "1. **remove_conjunctions_from_sets_boolean(input_list)**:\r\n",
    "   - This function takes a list of tuples `(sentence, info)` as input.\r\n",
    "   - It initializes a list `conjunctions` containing common conjunction words.\r\n",
    "   - It iterates through each tuple in the input list, where `s` represents the sentence and `x` represents some associated information.\r\n",
    "   - For each sentence, it iterates through its words:\r\n",
    "      - It checks if any prefix of the sentence matches with any conjunction in the `conjunctions` list. If a match is found, it removes the conjunction and its preceding words.\r\n",
    "      - It also checks if any suffix of the sentence matches with any conjunction. If found, it removes the conjunction and its succeeding words.\r\n",
    "   - It appends the modified sentence along with the associated information to the `filtered_list`.\r\n",
    "   - Finally, it returns the filtered list of tuples.\r\n",
    "\r\n",
    "2. **word_segment_boolean(sentences)**:\r\n",
    "   - This function takes a list of tuples `(sentence, info)` as input.\r\n",
    "   - It initializes an empty list `len_3_word` to store words with length less than 3.\r\n",
    "   - It initializes an empty list `segmented_sentences` to store segmented sentences.\r\n",
    "   - It initializes an empty list `term` to store the final terms after processing.\r\n",
    "   - For each sentence in the input list:\r\n",
    "      - It initializes an empty list `words` to store segmented words of the sentence.\r\n",
    "      - It iterates through the sentence character by character, attempting to find valid words by checking if substrings form valid English words.\r\n",
    "      - If a word is found and its length is greater than or equal to 4, it is added to the `words` list.\r\n",
    "      - If a word has a length less than 4, it is added to the `len_3_word` list.\r\n",
    "   - After segmenting each sentence, it constructs the `term` list by checking each word:\r\n",
    "      - It ensures the word is not a stop word, it's a valid English word, and its length is at least 3.\r\n",
    "      - It checks if the word is a noun using WordNet.\r\n",
    "      - If the conditions are met, it appends the word along with its associated information to the `term` list.\r\n",
    "   - Finally, it returns the `term` list containing segmther analysis or processing.code's functionality.ne the repository to your local machine:\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e8bfeb8d-43b9-4987-b2ce-c99cae53c96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set([\"a\", \"is\", \"the\", \"of\", \"all\", \"and\", \"to\", \"can\", \"be\", \"as\", \"once\", \"for\", \"at\", \"am\", \"are\", \"has\", \"have\", \"had\", \"up\", \"his\", \"her\", \"in\", \"on\", \"no\", \"we\", \"do\"])\n",
    "punctuation = set(['!', '\\\\', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '—'])\n",
    "english_words = set(nltk.corpus.words.words())\n",
    "porter = PorterStemmer()\n",
    "\n",
    "\n",
    "#remove_stopwords, simply remove the stopwords and the punctuatated tokens\n",
    "def remove_stopwords(tokens):    \n",
    "    punctuation = set(string.punctuation)\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words and word not in punctuation]\n",
    "    return filtered_tokens\n",
    "\n",
    "#comment 1\n",
    "\n",
    "def extract_and_index_data(directory):\n",
    "    boolean_token1 = []\n",
    "    positional_token1 = []\n",
    "    \n",
    "    boolean_token_of_len_2 = []\n",
    "    positional_token_of_len_2 = []\n",
    "    \n",
    "    boolean_token_as_sentence = []\n",
    "    positional_token_as_sentence = []\n",
    "    \n",
    "    boolean_token_have_hyphen = []\n",
    "    positional_token_have_hyphen = []\n",
    "    \n",
    "    boolean_token_have_punctuation = []\n",
    "    positional_token_have_punctuation = []\n",
    "    \n",
    "    boolean_all_token = []\n",
    "    positional_all_token = []\n",
    "\n",
    "    boolean_all_number = []\n",
    "    positional_all_number = []\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        index = 0\n",
    "        if os.path.isfile(os.path.join(directory, filename)):\n",
    "            with open(os.path.join(directory, filename), 'r') as text:\n",
    "                data = text.read()\n",
    "\n",
    "            file_tokens = word_tokenize(data)\n",
    "            normalized = remove_stopwords(file_tokens)\n",
    "\n",
    "            # index = 0\n",
    "            for token in normalized:\n",
    "                if 3 <= len(token) <= 14 and all(char not in string.punctuation for char in token) and not token.isdigit():\n",
    "                    boolean_token1.append((token.lower(), filename))\n",
    "                    positional_token1.append((token.lower(), filename, index))\n",
    "                elif len(token) == 2 and not token.isdigit():\n",
    "                    boolean_token_of_len_2.append((token.lower(), filename))\n",
    "                    positional_token_of_len_2.append((token.lower(), filename, index))\n",
    "                elif len(token) > 14 and '-' not in token and not token.isdigit():\n",
    "                    boolean_token_as_sentence.append((token.lower(), filename))\n",
    "                    positional_token_as_sentence.append((token.lower(), filename, index))\n",
    "                elif '-' in token:\n",
    "                    boolean_token_have_hyphen.append((token.lower(), filename))\n",
    "                    positional_token_have_hyphen.append((token.lower(), filename, index))\n",
    "                elif any(char in string.punctuation for char in token) and '-' not in token and len(token) <= 14 and not token.isdigit():\n",
    "                    boolean_token_have_punctuation.append((token.lower(), filename))\n",
    "                    positional_token_have_punctuation.append((token.lower(), filename, index))\n",
    "                elif token.isdigit():\n",
    "                    positional_all_number.append((token.lower(), filename, index))\n",
    "                    boolean_all_number.append((token.lower(), filename))\n",
    "                else:\n",
    "                    boolean_all_token.append((token.lower(), filename))\n",
    "                index += 1  # Adding 1 for the space after the token\n",
    "                positional_all_token.append((token.lower(), filename, index))\n",
    "\n",
    "    return (boolean_token1, boolean_token_of_len_2, boolean_token_as_sentence, boolean_token_have_hyphen, boolean_token_have_punctuation, boolean_all_token, boolean_all_number), \\\n",
    "           (positional_token1, positional_token_of_len_2, positional_token_as_sentence, positional_token_have_hyphen, positional_token_have_punctuation, positional_all_token, positional_all_number)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "directory_path = r'D:\\SEMESTER 06\\INFORMATION RETRIEVAL\\ASSIGNMENT I\\ResearchPapers'\n",
    "#function invoking\n",
    "boolean_tokens, positional_tokens = extract_and_index_data(directory_path)\n",
    "\n",
    "boolean_token1, boolean_token_of_len_2, boolean_token_as_sentence, boolean_token_have_hyphen, boolean_token_have_punctuation, boolean_all_token, boolean_all_number = boolean_tokens\n",
    "positional_token1, positional_token_of_len_2, positional_token_as_sentence, positional_token_have_hyphen, positional_token_have_punctuation, positional_all_token, positional_all_number = positional_tokens\n",
    "\n",
    "#########################   \n",
    "\n",
    "\n",
    "#COMMENT 2\n",
    "\n",
    "def cleaning_pipeline_boolean(token):\n",
    "    clean_token = []\n",
    "    token_split = []\n",
    "    new_token = []\n",
    "    tokens_no_further_processing_required = []\n",
    "    token_contain_only_numbers = []\n",
    "    for x, y in token:\n",
    "        if len(x) >= 4 and not x.isdigit() and any(c.isalpha() for c in x):\n",
    "            new_x = ''.join(' ' if char in punctuation else char for char in x)\n",
    "            if ' ' in new_x:\n",
    "                token_split = new_x.split(' ')\n",
    "                for word in token_split:\n",
    "                    if len(word) >= 4:\n",
    "                        new_token.append((word, y))\n",
    "            else:\n",
    "                if len(new_x) >= 4:\n",
    "                    new_token.append((new_x, y))\n",
    "        elif x.isdigit():\n",
    "            token_contain_only_numbers.append((x, y))\n",
    "    \n",
    "    for x,y in new_token:\n",
    "        if x.isdigit():\n",
    "            token_contain_only_numbers.append((x,y))\n",
    "\n",
    "    \n",
    "    for x, y in new_token:\n",
    "        if any(char.isdigit() for char in x):\n",
    "            cleaned_x = ''.join(char if char != '—' else '' for char in x) and ''.join(char for char in x if not char.isdigit())\n",
    "            split_tokens = nltk.word_tokenize(cleaned_x)\n",
    "            for word in split_tokens:\n",
    "                if len(word) >= 4:\n",
    "                    clean_token.append((word, y))\n",
    "        else:\n",
    "            clean_token.append((x, y))\n",
    "    tokens_no_further_processing_required = [(x,y) for x,y in clean_token if x in english_words]\n",
    "    clean_token = [(x, y) for x, y in clean_token if x not in english_words]\n",
    "    return tokens_no_further_processing_required,clean_token,token_contain_only_numbers\n",
    "\n",
    "\n",
    "def cleaning_pipeline_positional(token):\n",
    "    clean_token = []\n",
    "    token_split = []\n",
    "    new_token = []\n",
    "    tokens_no_further_processing_required = []\n",
    "    token_contain_only_numbers = []\n",
    "    for x, y, z in token:\n",
    "        if len(x) >= 4 and not x.isdigit() and any(c.isalpha() for c in x):\n",
    "            new_x = ''.join(' ' if char in punctuation else char for char in x)\n",
    "            if ' ' in new_x:\n",
    "                token_split = new_x.split(' ')\n",
    "                for word in token_split:\n",
    "                    if len(word) >= 4:\n",
    "                        new_token.append((word, y, z))\n",
    "            else:\n",
    "                if len(new_x) >= 4:\n",
    "                    new_token.append((new_x, y, z))\n",
    "        elif x.isdigit():\n",
    "            token_contain_only_numbers.append((x, y, z))\n",
    "    \n",
    "    for x,y,z in new_token:\n",
    "        if x.isdigit():\n",
    "            token_contain_only_numbers.append((x,y,z))\n",
    "\n",
    "    \n",
    "    for x, y, z in new_token:\n",
    "        if any(char.isdigit() for char in x):\n",
    "            cleaned_x = ''.join(char if char != '—' else '' for char in x) and ''.join(char for char in x if not char.isdigit())\n",
    "            split_tokens = nltk.word_tokenize(cleaned_x)\n",
    "            for word in split_tokens:\n",
    "                if len(word) >= 4:\n",
    "                    clean_token.append((word, y, z))\n",
    "        else:\n",
    "            clean_token.append((x, y, z))\n",
    "    tokens_no_further_processing_required = [(x,y, z) for x,y, z in clean_token if x in english_words]\n",
    "    clean_token = [(x, y, z) for x, y , z in clean_token if x not in english_words]\n",
    "    return tokens_no_further_processing_required,clean_token,token_contain_only_numbers\n",
    "\n",
    "\n",
    "\n",
    "#########################################\n",
    "\n",
    "#COMMENT 3\n",
    "\n",
    "def token_seperator_boolean(token):\n",
    "    words = []\n",
    "    for x in english_words:\n",
    "        words.append(x)\n",
    "\n",
    "    for x,y in boolean_token1:\n",
    "        words.append(x)\n",
    "\n",
    "\n",
    "    wordcost = {k: log((i + 1) * log(len(words))) for i, k in enumerate(words)}\n",
    "    maxword = max(len(x) for x in words)\n",
    "    # return infer_spaces(token,maxword,wordcost)\n",
    "    clean_term = []\n",
    "    for i in range(len(token)):\n",
    "        clean_term.append((infer_spaces_boolean(token[i][0].lower(),maxword,wordcost),token[i][1]))\n",
    "    return cleaning_pipeline_boolean(clean_term)\n",
    "    \n",
    "\n",
    "def infer_spaces_boolean(s,maxword,wordcost):\n",
    "    def best_match(i):\n",
    "        candidates = enumerate(reversed(cost[max(0, i - maxword):i]))\n",
    "        return min((c + wordcost.get(s[i - k - 1:i], 9e999), k + 1) for k, c in candidates)\n",
    "    \n",
    "    cost = [0]\n",
    "    for i in range(1, len(s) + 1):\n",
    "        c, k = best_match(i)\n",
    "        cost.append(c)\n",
    "    \n",
    "    out = []\n",
    "    i = len(s)\n",
    "    while i > 0:\n",
    "        c, k = best_match(i)\n",
    "        assert c == cost[i]\n",
    "        out.append(s[i - k:i])\n",
    "        i -= k\n",
    "    \n",
    "    return \" \".join(reversed(out))\n",
    "\n",
    "\n",
    "\n",
    "def token_seperator_poistional(token):\n",
    "    words = []\n",
    "    for x in english_words:\n",
    "        words.append(x)\n",
    "\n",
    "    for x, y, z in positional_token1:\n",
    "        words.append(x)\n",
    "\n",
    "    wordcost = {k: log((i + 1) * log(len(words))) for i, k in enumerate(words)}\n",
    "    maxword = max(len(x) for x in words)\n",
    "    clean_term = []\n",
    "    for i in range(len(token)):\n",
    "        clean_term.append((infer_spaces_positional(token[i][0].lower(), maxword, wordcost), token[i][1], token[i][2]))\n",
    "    return cleaning_pipeline_positional(clean_term)\n",
    "\n",
    "\n",
    "def infer_spaces_positional(s, maxword, wordcost):\n",
    "    def best_match(i):\n",
    "        candidates = enumerate(reversed(cost[max(0, i - maxword):i]))\n",
    "        return min((c + wordcost.get(s[i - k - 1:i], 9e999), k + 1) for k, c in candidates)\n",
    "\n",
    "    cost = [0]\n",
    "    for i in range(1, len(s) + 1):\n",
    "        c, k = best_match(i)\n",
    "        cost.append(c)\n",
    "\n",
    "    out = []\n",
    "    i = len(s)\n",
    "    while i > 0:\n",
    "        c, k = best_match(i)\n",
    "        assert c == cost[i]\n",
    "        out.append(s[i - k:i])\n",
    "        i -= k\n",
    "\n",
    "    return \" \".join(reversed(out))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#######################################\n",
    "\n",
    "#COMMENT 4\n",
    "\n",
    "def remove_conjunctions_from_sets_boolean(input_list):\n",
    "    conjunctions = ['this', 'that','of', 'and', 'or', 'but', 'for', 'nor', 'so', 'yet', 'to', 'with', 'in', 'on', 'at', 'by', 'is', \"the\", \"of\", \"all\", \"and\", \"to\", \"can\", \"be\", \"as\", \"once\", \"for\", \"at\", \"am\", \"are\", \"has\", \"have\", \"had\", \"up\", \"his\", \"her\", \"in\", \"on\", \"no\"]\n",
    "    filtered_list = []\n",
    "    for s, x in input_list:  # Unpack the tuple\n",
    "        # Check from the beginning\n",
    "        for i in range(len(s)):\n",
    "            first_word = s[:i].lower()\n",
    "            if first_word in conjunctions:\n",
    "                s = s[i:]\n",
    "                break\n",
    "        # Check from the end\n",
    "        for i in range(len(s), 0, -1):\n",
    "            last_word = s[i:].lower()\n",
    "            if last_word in conjunctions:\n",
    "                s = s[:i]\n",
    "                break\n",
    "        filtered_list.append((s, x))  # Append the modified tuple\n",
    "    return filtered_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def word_segment_boolean(sentences):\n",
    "    len_3_word = []\n",
    "    segmented_sentences = []\n",
    "    term = []\n",
    "    for sentence, info in sentences:\n",
    "        words = []\n",
    "        start = 0\n",
    "        while start < len(sentence):\n",
    "            found = False\n",
    "            for end in range(len(sentence), start, -1):\n",
    "                word = sentence[start:end]\n",
    "                if word.lower() in english_words:\n",
    "                    if word.lower().startswith(\"the\"):\n",
    "                        word = word[2:]\n",
    "                    if len(word) >= 4:\n",
    "                        if word.lower() not in {\"a\", \"an\", \"the\"} or start != 0:\n",
    "                            words.append(word)\n",
    "                        start = end\n",
    "                        found = True\n",
    "                    else:\n",
    "                        len_3_word.append((word, info))\n",
    "                    break\n",
    "            if not found:\n",
    "                start += 1\n",
    "        segmented_sentences.append((words, info))\n",
    "\n",
    "    term = [(wd.lower(), info) for wd, info in len_3_word if len(wd) >= 3 and wordnet.synsets(word, pos=wordnet.NOUN) and word.lower() not in stop_words and word not in punctuation]\n",
    "    for x, y in segmented_sentences:\n",
    "        for word in x:\n",
    "            if word not in stop_words and word in english_words:\n",
    "                term.append((word, y))\n",
    "    return term\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def remove_conjunctions_from_sets_positional(input_list):\n",
    "    conjunctions = ['this', 'that','of', 'and', 'or', 'but', 'for', 'nor', 'so', 'yet', 'to', 'with', 'in', 'on', 'at', 'by', 'is', \"the\", \"of\", \"all\", \"and\", \"to\", \"can\", \"be\", \"as\", \"once\", \"for\", \"at\", \"am\", \"are\", \"has\", \"have\", \"had\", \"up\", \"his\", \"her\", \"in\", \"on\", \"no\"]\n",
    "    filtered_list = []\n",
    "    for s, x, index in input_list:  # Unpack the tuple\n",
    "        # Check from the beginning\n",
    "        for i in range(len(s)):\n",
    "            first_word = s[:i].lower()\n",
    "            if first_word in conjunctions:\n",
    "                s = s[i:]\n",
    "                break\n",
    "        # Check from the end\n",
    "        for i in range(len(s), 0, -1):\n",
    "            last_word = s[i:].lower()\n",
    "            if last_word in conjunctions:\n",
    "                s = s[:i]\n",
    "                break\n",
    "        filtered_list.append((s, x, index))  # Append the modified tuple\n",
    "    return filtered_list\n",
    "\n",
    "\n",
    "def word_segment_positional(sentences):\n",
    "    len_3_word = []\n",
    "    segmented_sentences = []\n",
    "    term = []\n",
    "    for sentence, info, index in sentences:\n",
    "        words = []\n",
    "        start = 0\n",
    "        while start < len(sentence):\n",
    "            found = False\n",
    "            for end in range(len(sentence), start, -1):\n",
    "                word = sentence[start:end]\n",
    "                if word.lower() in english_words:\n",
    "                    if word.lower().startswith(\"the\"):\n",
    "                        word = word[2:]\n",
    "                    if len(word) >= 4:\n",
    "                        if word.lower() not in {\"a\", \"an\", \"the\"} or start != 0:\n",
    "                            words.append(word)\n",
    "                        start = end\n",
    "                        found = True\n",
    "                    else:\n",
    "                        len_3_word.append((word, info, index))\n",
    "                    break\n",
    "            if not found:\n",
    "                start += 1\n",
    "        segmented_sentences.append((words, info, index))\n",
    "\n",
    "    term = [(wd.lower(), info, index) for wd, info, index in len_3_word if len(wd) >= 3 and wordnet.synsets(word, pos=wordnet.NOUN) and word.lower() not in stop_words and word not in punctuation]\n",
    "    for x, y, index in segmented_sentences:\n",
    "        for word in x:\n",
    "            if word not in stop_words and word in english_words:\n",
    "                term.append((word, y, index))\n",
    "    return term\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "####################\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45521804-9014-420e-8844-5ce17a71c6f2",
   "metadata": {},
   "source": [
    "# INVERTED INDEX\n",
    "# COMMENT 5\n",
    "\n",
    "\r\n",
    "1. **Boolean Token Segregation and Cleaning**:\r\n",
    "   - The code starts by segregating different types of boolean tokens based on specific conditions like presence of hyphens, existence in vocabulary, presence of punctuation, etc.\r\n",
    "   - Tokens starting with URLs or domain names are separated out and stored in `boolean_token_link`. Other tokens are filtered accordingly.\r\n",
    "   - Tokens containing hyphens are also segregated, with those meeting certain length criteria and containing a single hyphen being stored in `boolean_hyphen_term`.\r\n",
    "   - Tokens existing in the English vocabulary are identified and stored in `boolean_token_exist_in_vocab`, while those not found in the vocabulary are filtered out.\r\n",
    "   - Additionally, tokens containing only numbers are processed separately and stored in `boolean_all_number`.\r\n",
    "   - The remaining tokens undergo a cleaning pipeline (`cleaning_pipeline_boolean`), which likely involves some form of normalization or preprocessing.\r\n",
    "   - Conjunctions are removed from the sets of cleaned tokens using `remove_conjunctions_from_sets_boolean`.\r\n",
    "   - Finally, the cleaned tokens are segmented into terms using `word_segment_boolean`.\r\n",
    "\r\n",
    "2. **Stemming and Inverted Index Creation**:\r\n",
    "   - Porter stemming is applied to the terms to reduce them to their root forms.\r\n",
    "   - Stemmed terms along with their corresponding information are stored in `stem_terms`.\r\n",
    "   - An inverted index is then created, where each term serves as a key, and the corresponding values are the positions where the term appears in the e processing system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4e80368b-9ab0-4168-94d6-ff2bb0019e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMMENT 5\n",
    "boolean_hyphen_term = []\n",
    "boolean_token_exist_in_vocab = []\n",
    "\n",
    "boolean_token_link = [(x, y) for x, y in boolean_token_have_hyphen if x.startswith('//') or x.startswith('www') or x.startswith('org')]\n",
    "boolean_token_have_hyphen = [(x, y) for x, y in boolean_token_have_hyphen if not (x.startswith('//') or x.startswith('www') or x.startswith('org'))]\n",
    "\n",
    "boolean_hyphen_term = [(x, y) for x, y in boolean_token_have_hyphen if x.count('-') == 1 and '-' not in [x[0], x[-1]] and 4 <= len(x.split('-')[0]) + len(x.split('-')[1]) <= 20]\n",
    "boolean_token_have_hyphen = [(x, y) for x, y in boolean_token_have_hyphen if (x, y) not in boolean_hyphen_term]\n",
    "\n",
    "boolean_token_link = [(x, y) for x, y in boolean_token_as_sentence if x.startswith('//') or x.startswith('www') or x.startswith('org')]\n",
    "boolean_token_as_sentence = [(x, y) for x, y in boolean_token_as_sentence if not (x.startswith('//') or x.startswith('www') or x.startswith('org'))]\n",
    "\n",
    "\n",
    "\n",
    "boolean_token_exist_in_vocab = [(x,y) for x,y in boolean_token_as_sentence if x in english_words]\n",
    "boolean_token_as_sentence = [(x, y) for x, y in boolean_token_as_sentence if x not in english_words]\n",
    "\n",
    "boolean_token1 += boolean_token_exist_in_vocab\n",
    "\n",
    "boolean_tokens_no_further_processing_required, boolean_clean_token, boolean_token_contain_only_numbers = cleaning_pipeline_boolean(boolean_token_as_sentence)\n",
    "boolean_all_number += boolean_token_contain_only_numbers\n",
    "boolean_token1 += boolean_tokens_no_further_processing_required\n",
    "\n",
    "\n",
    "boolean_tokens_no_further_processing_required1, boolean_clean_token1, boolean_token_contain_only_numbers1 = cleaning_pipeline_boolean(boolean_token_have_punctuation)\n",
    "boolean_all_number += boolean_token_contain_only_numbers1\n",
    "boolean_token1 += boolean_tokens_no_further_processing_required1\n",
    "boolean_clean_token += boolean_clean_token1\n",
    "\n",
    "tokens_no_further_processing_required3, boolean_clean_token3, token_contain_only_numbers3 = cleaning_pipeline_boolean(boolean_token_have_hyphen)\n",
    "boolean_all_number += token_contain_only_numbers3\n",
    "boolean_token1 += tokens_no_further_processing_required3\n",
    "boolean_clean_token += boolean_clean_token3\n",
    "\n",
    "boolean_tokens_no_further_processing_required2, boolean_clean_token2,token_contain_only_numbers2 = token_seperator_boolean(boolean_clean_token)\n",
    "boolean_token1 += boolean_tokens_no_further_processing_required2\n",
    "\n",
    "boolean_sets_list = remove_conjunctions_from_sets_boolean(boolean_clean_token2)\n",
    "boolean_token_sentences = word_segment_boolean(boolean_sets_list)\n",
    "\n",
    "boolean_term = []\n",
    "\n",
    "boolean_term = boolean_token_link + boolean_token1 + boolean_all_number + boolean_token_sentences + boolean_hyphen_term\n",
    "\n",
    "stem_terms = []\n",
    "for x, y in boolean_term:\n",
    "    stemmed_x = porter.stem(x)\n",
    "    stem_terms.append((stemmed_x,y))\n",
    "\n",
    "inverted_index = {}\n",
    "\n",
    "for x, y in stem_terms:\n",
    "    if x not in inverted_index:\n",
    "        inverted_index[x] = []\n",
    "    if y not in inverted_index[x]:\n",
    "        inverted_index[x].append(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50ea8df-2a2c-44b2-93de-1bccdc9654a1",
   "metadata": {},
   "source": [
    "# POSITIONAL INDEX\n",
    "\n",
    "# COMMENT 6\n",
    "\n",
    "\r\n",
    "1. **Positional Token Segregation and Cleaning**:\r\n",
    "   - Similar to before, the code segregates different types of positional tokens based on conditions like the presence of hyphens, existence in the vocabulary, and the presence of punctuation.\r\n",
    "   - Tokens starting with URLs or domain names are separated out and stored in `positional_token_link`. Other tokens are filtered accordingly.\r\n",
    "   - Tokens containing hyphens are also segregated based on specific length and structure criteria.\r\n",
    "   - Tokens existing in the English vocabulary are identified and stored in `positional_token_exist_in_vocab`, while those not found are filtered out.\r\n",
    "   - The tokens then undergo cleaning pipelines (`cleaning_pipeline_positional`) and are processed for further segmentation and cleaning.\r\n",
    "   - Finally, tokens are segmented into terms using `word_segment_positional`.\r\n",
    "\r\n",
    "2. **Stemming and Positional Index Creation**:\r\n",
    "   - Porter stemming is applied to the terms to reduce them to their root forms.\r\n",
    "   - Stemmed terms along with their corresponding document names and positions are stored in `stem_terms_positional`.\r\n",
    "   - A positional index is then created, where each document name serves as a key, and the corresponding values are dictionaries containing terms and their positions within td their positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e35cf31c-953b-4dcf-a7d6-7b2647304acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMMENT 6\n",
    "positional_hyphen_term = []\n",
    "positional_token_exist_in_vocab = []\n",
    "\n",
    "positional_token_link = [(x, y , z) for x, y, z in positional_token_have_hyphen if x.startswith('//') or x.startswith('www') or x.startswith('org')]\n",
    "positional_token_have_hyphen = [(x, y , z) for x, y, z in positional_token_have_hyphen if not (x.startswith('//') or x.startswith('www') or x.startswith('org'))]\n",
    "\n",
    "positional_hyphen_term = [(x, y , z) for x, y, z in positional_token_have_hyphen if x.count('-') == 1 and '-' not in [x[0], x[-1]] and 4 <= len(x.split('-')[0]) + len(x.split('-')[1]) <= 20]\n",
    "positional_token_have_hyphen = [(x, y , z) for x, y, z in positional_token_have_hyphen if (x, y) not in positional_hyphen_term]\n",
    "\n",
    "positional_token_link = [(x, y , z) for x, y, z in positional_token_as_sentence if x.startswith('//') or x.startswith('www') or x.startswith('org')]\n",
    "positional_token_as_sentence = [(x, y , z) for x, y, z in positional_token_as_sentence if not (x.startswith('//') or x.startswith('www') or x.startswith('org'))]\n",
    "\n",
    "positional_token_exist_in_vocab = [(x, y , z) for x, y, z in positional_token_as_sentence if x in english_words]\n",
    "positional_token_as_sentence = [(x, y , z) for x, y, z in positional_token_as_sentence if x not in english_words]\n",
    "\n",
    "positional_token1 += positional_token_exist_in_vocab\n",
    "\n",
    "positional_tokens_no_further_processing_required, positional_clean_token, positional_token_contain_only_numbers = cleaning_pipeline_positional(positional_token_as_sentence)\n",
    "positional_all_number += positional_token_contain_only_numbers\n",
    "positional_token1 += positional_tokens_no_further_processing_required\n",
    "\n",
    "positional_tokens_no_further_processing_required1, positional_clean_token1, positional_token_contain_only_numbers1 = cleaning_pipeline_positional(positional_token_have_punctuation)\n",
    "positional_all_number += positional_token_contain_only_numbers1\n",
    "positional_token1 += positional_tokens_no_further_processing_required1\n",
    "positional_clean_token += positional_clean_token1\n",
    "\n",
    "tokens_no_further_processing_required3, positional_clean_token3, token_contain_only_numbers3 = cleaning_pipeline_positional(positional_token_have_hyphen)\n",
    "positional_all_number += token_contain_only_numbers3\n",
    "positional_token1 += tokens_no_further_processing_required3\n",
    "positional_clean_token += positional_clean_token3\n",
    "\n",
    "positional_tokens_no_further_processing_required2, positional_clean_token2, token_contain_only_numbers2 = token_seperator_poistional(positional_clean_token)\n",
    "positional_token1 += positional_tokens_no_further_processing_required2\n",
    "\n",
    "positional_sets_list = remove_conjunctions_from_sets_positional(positional_clean_token2)\n",
    "positional_token_sentences = word_segment_positional(positional_sets_list)\n",
    "\n",
    "positional_term = []\n",
    "\n",
    "positional_term = positional_token_link + positional_token1 + positional_all_number + positional_token_sentences + positional_hyphen_term\n",
    "\n",
    "porter = PorterStemmer()\n",
    "stem_terms_positional = []\n",
    "for x, y, z in positional_term:\n",
    "    stemmed_x = porter.stem(x)\n",
    "    stem_terms_positional.append((stemmed_x,y,z))\n",
    "\n",
    "positional_index = {}\n",
    "    \n",
    "for term, doc_name, index in stem_terms_positional:\n",
    "    if doc_name not in positional_index:\n",
    "        positional_index[doc_name] = {}\n",
    "    \n",
    "    if term not in positional_index[doc_name]:\n",
    "        positional_index[doc_name][term] = []\n",
    "    \n",
    "    positional_index[doc_name][term].append(index)\n",
    "    positional_index[doc_name][term].sort()  # Sort the indexes in ascending order\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7047d5-ed0f-4163-853f-8a3f77086a9a",
   "metadata": {},
   "source": [
    "# SEARCH ENGINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eaae9ff3-5b10-4d6c-8986-f8ff4fc31075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolean_search(boolean_query):\n",
    "    porter = PorterStemmer()\n",
    "    query_terms = boolean_query.split()\n",
    "    stemmed_query_terms = []\n",
    "    doc = {'1.txt', '2.txt', '3.txt', '7.txt', '8.txt', '9.txt', '11.txt', '12.txt', '13.txt', '14.txt', '15.txt', '16.txt', '17.txt', '18.txt', '21.txt', '22.txt', '23.txt', '24.txt', '25.txt', '26.txt'}\n",
    "\n",
    "    for term in query_terms:\n",
    "        stemmed_term = porter.stem(term.lower())\n",
    "        stemmed_query_terms.append(stemmed_term)\n",
    "\n",
    "    common_docs = set(inverted_index.get(stemmed_query_terms[0], []))\n",
    "\n",
    "    i = 0\n",
    "    while i < len(stemmed_query_terms)-1:\n",
    "        \n",
    "        if stemmed_query_terms[i] == 'not':\n",
    "            term = stemmed_query_terms[i+1]\n",
    "            term_docs = set(inverted_index.get(term, []))\n",
    "            common_docs = doc.difference(term_docs)\n",
    "            i += 2\n",
    "\n",
    "        elif i+1 < len(stemmed_query_terms) and stemmed_query_terms[i+1] == 'not':\n",
    "            term = stemmed_query_terms[i+2]\n",
    "            operator = query_terms[i].lower()\n",
    "            term_docs = set(inverted_index.get(term, []))\n",
    "            term_docs_complement = doc.difference(term_docs)\n",
    "            \n",
    "            if operator == 'and':\n",
    "                common_docs = common_docs.intersection(term_docs_complement)\n",
    "            elif operator == 'or':\n",
    "                common_docs = common_docs.union(term_docs_complement)\n",
    "            i += 3\n",
    "\n",
    "        else:\n",
    "            operator = query_terms[i+1].lower()\n",
    "            term_docs = set(inverted_index.get(stemmed_query_terms[i+2], []))\n",
    "            \n",
    "            if operator == 'and':\n",
    "                common_docs = common_docs.intersection(term_docs)\n",
    "            elif operator == 'or':\n",
    "                common_docs = common_docs.union(term_docs)\n",
    "            i += 2\n",
    "\n",
    "\n",
    "    return list(common_docs)\n",
    "\n",
    "\n",
    "def proximity_search(term1, term2, proximity):\n",
    "    result = set()\n",
    "    for doc_name in positional_index:\n",
    "        if term1 in positional_index[doc_name] and term2 in positional_index[doc_name]:\n",
    "            indexes1 = positional_index[doc_name][term1]\n",
    "            indexes2 = positional_index[doc_name][term2]\n",
    "            for index1 in indexes1:\n",
    "                for index2 in indexes2:\n",
    "                    if abs(index1 - index2) <= proximity:\n",
    "                        result.add(doc_name)\n",
    "                        break  # No need to check other occurrences of term1 in this document\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb86db6-3a66-459b-afd8-b20753a8b84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your search query (or 'press enter, pass empty srting' to quit):  heart\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: heart\n",
      "Documents found: ['11.txt', '3.txt', '1.txt', '8.txt', '9.txt', '26.txt', '7.txt']\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your search query (or 'press enter, pass empty srting' to quit):  past research /3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: past research /3\n",
      "past research 3\n",
      "Documents found: {'12.txt'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def search_documents(query):\n",
    "    # Check if the query is empty\n",
    "    if not query:\n",
    "        print(\"Please enter a valid search query.\")\n",
    "        return\n",
    "    \n",
    "    if '/' in query:\n",
    "        terms, proximity = query.split('/')\n",
    "        term1, term2 = terms.split()\n",
    "        proximity = int(proximity)\n",
    "        print(term1, term2, proximity)\n",
    "        results = proximity_search(term1, term2, proximity)\n",
    "    else:\n",
    "        results = boolean_search(query)\n",
    "    \n",
    "    if results:\n",
    "        print(\"Documents found:\", results)\n",
    "    else:\n",
    "        print(\"No documents found for the\",query)\n",
    "\n",
    "\n",
    "while True:\n",
    "    query = input(\"Enter your search query (or 'press enter, pass empty srting' to quit): \")\n",
    "    if query.lower() == '':\n",
    "        break\n",
    "    print(\"Query:\", query)\n",
    "    search_documents(query)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23d3a34-03a6-46b9-8023-3f9a6ef6a921",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
