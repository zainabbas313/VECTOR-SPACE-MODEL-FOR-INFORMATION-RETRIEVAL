{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8860b3a6-6ea5-434a-8e35-a33e7bd88ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "from math import log\n",
    "import csv\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51abd3bc-03f2-488e-8258-d4675309c1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Zain\n",
      "[nltk_data]     Abbas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to C:\\Users\\Zain\n",
      "[nltk_data]     Abbas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192a5f6a-7f3e-47f3-a6a8-949e570ba763",
   "metadata": {},
   "source": [
    "# Text Data Processing Toolkit\r\n",
    "\r\n",
    "This toolkit provides functions to process text data extracted from research papers. It includes functions for tokenization, cleaning, and segmentation of text.\r\n",
    "\r\n",
    "## Functions\r\n",
    "\r\n",
    "### 1. `remove_stopwords(tokens)`\r\n",
    "\r\n",
    "- **Purpose**: Removes stopwords and punctuation from a list of tokens.\r\n",
    "- **Input**: \r\n",
    "  - `tokens`: List of tokens to be processed.\r\n",
    "- **Output**:\r\n",
    "  - `filtered_tokens`: List of tokens with stopwords and punctuation removed.\r\n",
    "\r\n",
    "### 2. `extract_and_index_data(directory)`\r\n",
    "\r\n",
    "- **Purpose**: Extracts and indexes data from text files in a directory.\r\n",
    "- **Input**:\r\n",
    "  - `directory`: Path to the directory containing text files.\r\n",
    "- **Output**:\r\n",
    "  - Categorized tokens:\r\n",
    "    - `token1`: Tokens of length 3-14 characters.\r\n",
    "    - `token_of_len_2`: Tokens of length 2 characters.\r\n",
    "    - `token_as_sentence`: Tokens longer than 14 characters.\r\n",
    "    - `token_have_hyphen`: Tokens containing hyphens.\r\n",
    "    - `token_have_punctuation`: Tokens containing punctuation.\r\n",
    "    - `all_token`: All other tokens.\r\n",
    "    - `all_number`: Tokens consisting only of numbers.\r\n",
    "\r\n",
    "### 3. `cleaning_pipeline(token)`\r\n",
    "\r\n",
    "- **Purpose**: Further cleans tokenized data by splitting tokens with multiple words, removing non-alphabetic characters, and filtering tokens based on their length and presence in the English dictionary.\r\n",
    "- **Input**:\r\n",
    "  - `token`: List of tokens to be cleaned.\r\n",
    "- **Output**:\r\n",
    "  - `tokens_no_further_processing_required`: Cleaned tokens not requiring further processing.\r\n",
    "  - `clean_token`: Cleaned tokens after initial processing.\r\n",
    "  - `token_contain_only_numbers`: Tokens containing only numbers.\r\n",
    "\r\n",
    "### 4. `token_seperator(token)`\r\n",
    "\r\n",
    "- **Purpose**: Separates concatenated tokens using dynamic programming and further cleans the tokens.\r\n",
    "- **Input**:\r\n",
    "  - `token`: List of tokens to be separated.\r\n",
    "- **Output**:\r\n",
    "  - `tokens_no_further_processing_required`: Cleaned tokens not requiring further processing.\r\n",
    "  - `clean_token`: Cleaned tokens after initial processing.\r\n",
    "  - `token_contain_only_numbers`: Tokens containing only numbers.\r\n",
    "\r\n",
    "### 5. `remove_conjunctions_from_sets(input_list)`\r\n",
    "\r\n",
    "- **Purpose**: Removes common conjunctions from a list of tokens.\r\n",
    "- **Input**:\r\n",
    "  - `input_list`: List of tokens to be processed.\r\n",
    "- **Output**:\r\n",
    "  - `filtered_list`: List of tokens with common conjunctions removed.\r\n",
    "\r\n",
    "### 6. `word_segment(sentences)`\r\n",
    "\r\n",
    "- **Purpose**: Segments words from sentences, handling cases of concatenated words and filtering out stopwords and short words.\r\n",
    "- **Input**:\r\n",
    "  - `sentences`: List of sentences to be processed.\r\n",
    "- **Output**:\r\n",
    "  - Segmented words.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10beb854-fa79-41a0-96ab-24423e800582",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set([\"a\", \"is\", \"the\", \"of\", \"all\", \"and\", \"to\", \"can\", \"be\", \"as\", \"once\", \"for\", \"at\", \"am\", \"are\", \"has\", \"have\", \"had\", \"up\", \"his\", \"her\", \"in\", \"on\", \"no\", \"we\", \"do\"])\n",
    "punctuation = set(['!', '\\\\', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '—'])\n",
    "english_words = set(nltk.corpus.words.words())\n",
    "porter = PorterStemmer()\n",
    "\n",
    "\n",
    "#remove_stopwords, simply remove the stopwords and the punctuatated tokens\n",
    "def remove_stopwords(tokens):    \n",
    "    punctuation = set(string.punctuation)\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words and word not in punctuation]\n",
    "    return filtered_tokens\n",
    "\n",
    "def extract_and_index_data(directory):\n",
    "    \n",
    "    token1 = []\n",
    "    token_of_len_2 = []\n",
    "    token_as_sentence = []\n",
    "    token_have_hyphen = []\n",
    "    token_have_punctuation = []\n",
    "    all_token = []\n",
    "    all_number = []\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "  \n",
    "        if os.path.isfile(os.path.join(directory, filename)):\n",
    "            with open(os.path.join(directory, filename), 'r') as text:\n",
    "                data = text.read()\n",
    "\n",
    "            file_tokens = word_tokenize(data)\n",
    "            normalized = remove_stopwords(file_tokens)\n",
    "\n",
    "            for token in normalized:\n",
    "                if 3 <= len(token) <= 14 and all(char not in string.punctuation for char in token) and not token.isdigit():\n",
    "                    token1.append((token.lower(), filename))\n",
    "                elif len(token) == 2 and not token.isdigit():\n",
    "                    token_of_len_2.append((token.lower(), filename))\n",
    "                elif len(token) > 14 and '-' not in token and not token.isdigit():\n",
    "                    token_as_sentence.append((token.lower(), filename))\n",
    "                elif '-' in token:\n",
    "                    token_have_hyphen.append((token.lower(), filename))\n",
    "                elif any(char in string.punctuation for char in token) and '-' not in token and len(token) <= 14 and not token.isdigit():\n",
    "                    token_have_punctuation.append((token.lower(), filename))\n",
    "                elif token.isdigit():\n",
    "                    all_number.append((token.lower(), filename))\n",
    "                else:\n",
    "                    all_token.append((token.lower(), filename))\n",
    "\n",
    "    return (token1, token_of_len_2, token_as_sentence, token_have_hyphen, token_have_punctuation, all_token, all_number)\n",
    "\n",
    "\n",
    "directory_path = r'D:\\SEMESTER 06\\INFORMATION RETRIEVAL\\ASSIGNMENT II\\ResearchPapers'\n",
    "#function invoking\n",
    "tokens= extract_and_index_data(directory_path)\n",
    "\n",
    "token1, token_of_len_2, token_as_sentence, token_have_hyphen, token_have_punctuation, all_token, all_number = tokens\n",
    "\n",
    "#########################   \n",
    "\n",
    "def cleaning_pipeline(token):\n",
    "    clean_token = []\n",
    "    token_split = []\n",
    "    new_token = []\n",
    "    tokens_no_further_processing_required = []\n",
    "    token_contain_only_numbers = []\n",
    "    for x, y in token:\n",
    "        if len(x) >= 4 and not x.isdigit() and any(c.isalpha() for c in x):\n",
    "            new_x = ''.join(' ' if char in punctuation else char for char in x)\n",
    "            if ' ' in new_x:\n",
    "                token_split = new_x.split(' ')\n",
    "                for word in token_split:\n",
    "                    if len(word) >= 4:\n",
    "                        new_token.append((word, y))\n",
    "            else:\n",
    "                if len(new_x) >= 4:\n",
    "                    new_token.append((new_x, y))\n",
    "        elif x.isdigit():\n",
    "            token_contain_only_numbers.append((x, y))\n",
    "    \n",
    "    for x,y in new_token:\n",
    "        if x.isdigit():\n",
    "            token_contain_only_numbers.append((x,y))\n",
    "\n",
    "    \n",
    "    for x, y in new_token:\n",
    "        if any(char.isdigit() for char in x):\n",
    "            cleaned_x = ''.join(char if char != '—' else '' for char in x) and ''.join(char for char in x if not char.isdigit())\n",
    "            split_tokens = nltk.word_tokenize(cleaned_x)\n",
    "            for word in split_tokens:\n",
    "                if len(word) >= 4:\n",
    "                    clean_token.append((word, y))\n",
    "        else:\n",
    "            clean_token.append((x, y))\n",
    "    tokens_no_further_processing_required = [(x,y) for x,y in clean_token if x in english_words]\n",
    "    clean_token = [(x, y) for x, y in clean_token if x not in english_words]\n",
    "    return tokens_no_further_processing_required,clean_token,token_contain_only_numbers\n",
    "\n",
    "#########################################\n",
    "\n",
    "def token_seperator(token):\n",
    "    words = []\n",
    "    for x in english_words:\n",
    "        words.append(x)\n",
    "\n",
    "    for x,y in token1:\n",
    "        words.append(x)\n",
    "\n",
    "\n",
    "    wordcost = {k: log((i + 1) * log(len(words))) for i, k in enumerate(words)}\n",
    "    maxword = max(len(x) for x in words)\n",
    "    # return infer_spaces(token,maxword,wordcost)\n",
    "    clean_term = []\n",
    "    for i in range(len(token)):\n",
    "        clean_term.append((infer_spaces(token[i][0].lower(),maxword,wordcost),token[i][1]))\n",
    "    return cleaning_pipeline(clean_term)\n",
    "    \n",
    "\n",
    "def infer_spaces(s,maxword,wordcost):\n",
    "    def best_match(i):\n",
    "        candidates = enumerate(reversed(cost[max(0, i - maxword):i]))\n",
    "        return min((c + wordcost.get(s[i - k - 1:i], 9e999), k + 1) for k, c in candidates)\n",
    "    \n",
    "    cost = [0]\n",
    "    for i in range(1, len(s) + 1):\n",
    "        c, k = best_match(i)\n",
    "        cost.append(c)\n",
    "    \n",
    "    out = []\n",
    "    i = len(s)\n",
    "    while i > 0:\n",
    "        c, k = best_match(i)\n",
    "        assert c == cost[i]\n",
    "        out.append(s[i - k:i])\n",
    "        i -= k\n",
    "    \n",
    "    return \" \".join(reversed(out))\n",
    "    \n",
    "#######################################\n",
    "\n",
    "\n",
    "def remove_conjunctions_from_sets(input_list):\n",
    "    conjunctions = ['this', 'that','of', 'and', 'or', 'but', 'for', 'nor', 'so', 'yet', 'to', 'with', 'in', 'on', 'at', 'by', 'is', \"the\", \"of\", \"all\", \"and\", \"to\", \"can\", \"be\", \"as\", \"once\", \"for\", \"at\", \"am\", \"are\", \"has\", \"have\", \"had\", \"up\", \"his\", \"her\", \"in\", \"on\", \"no\"]\n",
    "    filtered_list = []\n",
    "    for s, x in input_list:  # Unpack the tuple\n",
    "        # Check from the beginning\n",
    "        for i in range(len(s)):\n",
    "            first_word = s[:i].lower()\n",
    "            if first_word in conjunctions:\n",
    "                s = s[i:]\n",
    "                break\n",
    "        # Check from the end\n",
    "        for i in range(len(s), 0, -1):\n",
    "            last_word = s[i:].lower()\n",
    "            if last_word in conjunctions:\n",
    "                s = s[:i]\n",
    "                break\n",
    "        filtered_list.append((s, x))  # Append the modified tuple\n",
    "    return filtered_list\n",
    "\n",
    "\n",
    "def word_segment(sentences):\n",
    "    len_3_word = []\n",
    "    segmented_sentences = []\n",
    "    term = []\n",
    "    for sentence, info in sentences:\n",
    "        words = []\n",
    "        start = 0\n",
    "        while start < len(sentence):\n",
    "            found = False\n",
    "            for end in range(len(sentence), start, -1):\n",
    "                word = sentence[start:end]\n",
    "                if word.lower() in english_words:\n",
    "                    if word.lower().startswith(\"the\"):\n",
    "                        word = word[2:]\n",
    "                    if len(word) >= 4:\n",
    "                        if word.lower() not in {\"a\", \"an\", \"the\"} or start != 0:\n",
    "                            words.append(word)\n",
    "                        start = end\n",
    "                        found = True\n",
    "                    else:\n",
    "                        len_3_word.append((word, info))\n",
    "                    break\n",
    "            if not found:\n",
    "                start += 1\n",
    "        segmented_sentences.append((words, info))\n",
    "\n",
    "    term = [(wd.lower(), info) for wd, info in len_3_word if len(wd) >= 3 and wordnet.synsets(word, pos=wordnet.NOUN) and word.lower() not in stop_words and word not in punctuation]\n",
    "    for x, y in segmented_sentences:\n",
    "        for word in x:\n",
    "            if word not in stop_words and word in english_words:\n",
    "                term.append((word, y))\n",
    "    return term\n",
    "\n",
    "####################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d0f1ea-e18c-4128-9fe5-b92607cadf29",
   "metadata": {},
   "source": [
    "# Additional Token Processing\n",
    "\n",
    "## Hyphenated Terms and Links\n",
    "\n",
    "Hyphenated terms and links are separated and filtered from the main token list.\n",
    "\n",
    "- **Hyphenated Terms**:\n",
    "  - Terms containing a single hyphen are extracted and checked for length and hyphen position.\n",
    "  - The extracted terms are added to the `hyphen_term` list.\n",
    "  \n",
    "- **Links**:\n",
    "  - Tokens starting with common link prefixes like 'http://', 'www', or 'org' are filtered and added to the `token_link` list.\n",
    "\n",
    "## Further Token Filtering\n",
    "\n",
    "Additional token filtering is performed to refine the token lists.\n",
    "\n",
    "- **Tokens with Hyphens**:\n",
    "  - Tokens separated in the previous step are removed from the main token list.\n",
    "\n",
    "- **Tokens Considered as Sentences**:\n",
    "  - Tokens resembling sentences, often starting with link prefixes, are filtered out.\n",
    "\n",
    "## Vocabulary Check\n",
    "\n",
    "Tokens are checked against an English vocabulary to filter out non-English words.\n",
    "\n",
    "- **Existing in Vocabulary**:\n",
    "  - Tokens found in the English vocabulary are separated into a new list (`token_exist_in_vocab`), while others remain in the `token_as_sentence` list.\n",
    "\n",
    "## Cleaning and Processing\n",
    "\n",
    "The remaining tokens undergo further cleaning and processing.\n",
    "\n",
    "- **Cleaning Pipeline**:\n",
    "  - The `cleaning_pipeline` function is applied to tokens categorized as sentences and tokens with punctuation, resulting in cleaned tokens.\n",
    "\n",
    "- **Token Separation**:\n",
    "  - The `token_seperator` function is used to separate tokens that might be concatenated words.\n",
    "\n",
    "## Stemming\n",
    "\n",
    "Finally, stemming is applied to the tokens, reducing them to their root forms.\n",
    "\n",
    "- **Stemming**:\n",
    "  - Each token is stemmed using a stemming algorithm (e.g., Porter Stemmer), and the stemmed tokens are collected in the `stem_terms` list.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "864b2550-9de3-4d31-b3dc-3af2ed5da886",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyphen_term = []\n",
    "token_exist_in_vocab = []\n",
    "\n",
    "token_link = [(x, y) for x, y in token_have_hyphen if x.startswith('//') or x.startswith('www') or x.startswith('org')]\n",
    "token_have_hyphen = [(x, y) for x, y in token_have_hyphen if not (x.startswith('//') or x.startswith('www') or x.startswith('org'))]\n",
    "\n",
    "hyphen_term = [(x, y) for x, y in token_have_hyphen if x.count('-') == 1 and '-' not in [x[0], x[-1]] and 4 <= len(x.split('-')[0]) + len(x.split('-')[1]) <= 20]\n",
    "token_have_hyphen = [(x, y) for x, y in token_have_hyphen if (x, y) not in hyphen_term]\n",
    "\n",
    "token_link = [(x, y) for x, y in token_as_sentence if x.startswith('//') or x.startswith('www') or x.startswith('org')]\n",
    "token_as_sentence = [(x, y) for x, y in token_as_sentence if not (x.startswith('//') or x.startswith('www') or x.startswith('org'))]\n",
    "\n",
    "\n",
    "\n",
    "token_exist_in_vocab = [(x,y) for x,y in token_as_sentence if x in english_words]\n",
    "token_as_sentence = [(x, y) for x, y in token_as_sentence if x not in english_words]\n",
    "\n",
    "token1 += token_exist_in_vocab\n",
    "\n",
    "tokens_no_further_processing_required, clean_token, token_contain_only_numbers = cleaning_pipeline(token_as_sentence)\n",
    "all_number += token_contain_only_numbers\n",
    "token1 += tokens_no_further_processing_required\n",
    "\n",
    "\n",
    "tokens_no_further_processing_required1, clean_token1, token_contain_only_numbers1 = cleaning_pipeline(token_have_punctuation)\n",
    "all_number += token_contain_only_numbers1\n",
    "token1 += tokens_no_further_processing_required1\n",
    "clean_token += clean_token1\n",
    "\n",
    "tokens_no_further_processing_required3, clean_token3, token_contain_only_numbers3 = cleaning_pipeline(token_have_hyphen)\n",
    "all_number += token_contain_only_numbers3\n",
    "token1 += tokens_no_further_processing_required3\n",
    "clean_token += clean_token3\n",
    "\n",
    "tokens_no_further_processing_required2, clean_token2,token_contain_only_numbers2 = token_seperator(clean_token)\n",
    "token1 += tokens_no_further_processing_required2\n",
    "\n",
    "sets_list = remove_conjunctions_from_sets(clean_token2)\n",
    "token_sentences = word_segment(sets_list)\n",
    "\n",
    "term = []\n",
    "\n",
    "term = token_link + token1 + all_number + token_sentences + hyphen_term\n",
    "\n",
    "stem_terms = []\n",
    "for x, y in term:\n",
    "    stemmed_x = porter.stem(x)\n",
    "    stem_terms.append((stemmed_x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96b2d004-225a-4c1e-b35d-0a1e3ee98365",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = []\n",
    "for x, y in stem_terms:\n",
    "    if y not in doc:\n",
    "        doc.append(y.split()[0])\n",
    "\n",
    "def natural_sort_key(s):\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split('([0-9]+)', s)]\n",
    "doc.sort(key=natural_sort_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4fdc3f2-b7aa-438d-9b05-361fec63066c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = {file: [] for file in doc}\n",
    "\n",
    "for term, file in stem_terms:\n",
    "    inverted_index[file].append(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25e1a5cb-1e85-4758-8e9b-1442ea93bbaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>//portal.acm.org/citation.cfm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>//doi.org/10.1109/cvprw.2009.5206848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>//doi.org/10.1109/cvprw.2009.5206848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>//doi.org/10.23919/mipro.2018.8400040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>//www.youtube.com/watch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       0\n",
       "0          //portal.acm.org/citation.cfm\n",
       "1   //doi.org/10.1109/cvprw.2009.5206848\n",
       "2   //doi.org/10.1109/cvprw.2009.5206848\n",
       "3  //doi.org/10.23919/mipro.2018.8400040\n",
       "4                //www.youtube.com/watch"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.max_rows', None)\n",
    "df = pd.DataFrame(inverted_index[doc[2]])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8733e3-4c78-4a7a-ad3c-f54d74383f24",
   "metadata": {},
   "source": [
    "# BOW: BAG OF WORDS\n",
    "## Bag-of-Words Representation\r\n",
    "\r\n",
    "This section describes the creation of a Bag-of-Words (BoW) representation from the processed token data.\r\n",
    "\r\n",
    "## Construction of BoW DataFrame\r\n",
    "\r\n",
    "A BoW DataFrame (`bow_df`) is constructed to represent the occurrence of terms across documents.\r\n",
    "\r\n",
    "- **DataFrame Structure**:\r\n",
    "  - Each row represents a document.\r\n",
    "  - Each column represents a unique stemmed term extracted from the documents.\r\n",
    "\r\n",
    "## Generating BoW Data\r\n",
    "\r\n",
    "The BoW DataFrame is populated with term frequencies for each document.\r\n",
    "\r\n",
    "- **Term Frequencies**:\r\n",
    "  - For each document (`file`), a Counter object is created to count the occurrences of terms.\r\n",
    "  - The frequency of each term in the document is recorded in the corresponding column of the DataFrame.\r\n",
    "\r\n",
    "## Filling Missing Values\r\n",
    "\r\n",
    "Any missing values (NaN) in the DataFrame are filled with zeros to represent terms that do not occur in certain documents.\r\n",
    "\r\n",
    "- **Handling Missing Values**:\r\n",
    "  - Missing values in the DataFrame are replaced with zeros using the `fillna` method, ensuring consistent representation across all documents.\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6665321-1ecf-4d56-bee4-9a3b13b1a31e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_terms = sorted(set(x for x,y in stem_terms))\n",
    "bow_df = pd.DataFrame(columns=all_terms, index=doc)\n",
    "\n",
    "for file, terms in inverted_index.items():\n",
    "    term_counts = Counter(terms)\n",
    "    bow_df.loc[file] = [term_counts[term] for term in all_terms]\n",
    "\n",
    "bow_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4351a530-00dd-4dfc-a158-c5ee3350152a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1.txt</th>\n",
       "      <th>2.txt</th>\n",
       "      <th>3.txt</th>\n",
       "      <th>7.txt</th>\n",
       "      <th>8.txt</th>\n",
       "      <th>9.txt</th>\n",
       "      <th>11.txt</th>\n",
       "      <th>12.txt</th>\n",
       "      <th>13.txt</th>\n",
       "      <th>14.txt</th>\n",
       "      <th>15.txt</th>\n",
       "      <th>16.txt</th>\n",
       "      <th>17.txt</th>\n",
       "      <th>18.txt</th>\n",
       "      <th>21.txt</th>\n",
       "      <th>22.txt</th>\n",
       "      <th>23.txt</th>\n",
       "      <th>24.txt</th>\n",
       "      <th>25.txt</th>\n",
       "      <th>26.txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71331005</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717–727</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71:2668-79</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71–101</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71–104</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71–80</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71–82</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71–83</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           1.txt 2.txt 3.txt 7.txt 8.txt 9.txt 11.txt 12.txt 13.txt 14.txt  \\\n",
       "71331005       0     0     0     0     0     0      0      0      0      0   \n",
       "717–727        0     0     0     0     0     0      0      1      0      0   \n",
       "71:2668-79     0     0     0     0     1     0      0      0      0      0   \n",
       "71–101         0     0     0     1     0     0      0      0      0      0   \n",
       "71–104         0     0     0     0     0     0      0      1      0      0   \n",
       "71–80          0     0     0     0     0     0      0      0      0      0   \n",
       "71–82          0     0     0     0     0     0      0      1      0      0   \n",
       "71–83          0     0     0     0     0     0      0      1      0      0   \n",
       "72             0     1     0     8     3     0      0      0      0      0   \n",
       "720            1     0     0     0     0     0      0      0      0      0   \n",
       "\n",
       "           15.txt 16.txt 17.txt 18.txt 21.txt 22.txt 23.txt 24.txt 25.txt  \\\n",
       "71331005        0      0      0      0      0      0      0      1      0   \n",
       "717–727         0      0      0      0      0      0      0      0      0   \n",
       "71:2668-79      0      0      0      0      0      0      0      0      0   \n",
       "71–101          0      0      0      0      0      0      0      0      0   \n",
       "71–104          0      0      0      0      0      0      0      0      0   \n",
       "71–80           0      1      0      0      0      0      0      0      0   \n",
       "71–82           0      0      0      0      0      0      0      0      0   \n",
       "71–83           0      0      0      0      0      0      0      0      0   \n",
       "72              0      2      0      0      0      0      0      1      0   \n",
       "720             0      0      0      0      2      0      0      0      0   \n",
       "\n",
       "           26.txt  \n",
       "71331005        0  \n",
       "717–727         0  \n",
       "71:2668-79      0  \n",
       "71–101          0  \n",
       "71–104          0  \n",
       "71–80           0  \n",
       "71–82           0  \n",
       "71–83           0  \n",
       "72              1  \n",
       "720             0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = bow_df.T\n",
    "bow.iloc[10000:10010]\n",
    "bow.iloc[2190:2200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a773fd8-bb45-41c2-85dd-27edb7fcce3a",
   "metadata": {},
   "source": [
    "# TF: TERM FREQUENCY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ea4d6f-3b70-4336-958b-1da05d7ea11e",
   "metadata": {},
   "source": [
    "## Construction of TF Matrix\n",
    "\n",
    "A TF matrix (`tf_matrix`) is constructed to represent the term frequencies normalized by document length.\n",
    "\n",
    "- **Matrix Structure**:\n",
    "  - Each row represents a document.\n",
    "  - Each column represents a unique stemmed term extracted from the documents.\n",
    "\n",
    "## Generating TF Data\n",
    "\n",
    "The TF matrix is populated with term frequencies normalized by document length.\n",
    "\n",
    "- **Normalization Formula**:\n",
    "  - Term frequencies are transformed using the formula: \\( 1 + \\log_{10}(tf_{ij} + 1) \\), where \\( tf_{ij} \\) is the term frequency of term \\( j \\) in document \\( i \\).\n",
    "\n",
    "- **Normalization Process**:\n",
    "  - For each term in each document, the term frequency is normalized using the logarithmic transformation.\n",
    "  - The normalized term frequencies are recorded in the corresponding cells of the TF matrix.\n",
    "\n",
    "## Filling Missing Values\n",
    "\n",
    "Any missing values (NaN) in the TF matrix are filled with zeros.\n",
    "\n",
    "- **Handling Missing Values**:\n",
    "  - Missing values in the TF matrix are replaced with zeros using the `fillna` method, ensuring consistent representation of term frequencies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09b01061-1210-438b-808e-2a4a6b5ed833",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf_matrix = pd.DataFrame(columns=all_terms, index=doc)\n",
    "tf_matrix = bow.copy()\n",
    "for i in range(len(all_terms)):\n",
    "    for j in range(20):\n",
    "        tf_matrix.iloc[i,j] = 1 + np.log10(bow.iloc[i,j] + 1)\n",
    "\n",
    "tf_matrix.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651c553c-c006-4403-ae99-cefa9194a112",
   "metadata": {},
   "source": [
    "# Explanation of Term Frequency Calculation\n",
    "\n",
    "The formula `1 + np.log10(bow.iloc[i,j] + 1)` is used to calculate the Term Frequency (TF) for each term in the Bag-of-Words (BoW) representation.\n",
    "\n",
    "## TF Calculation Formula\n",
    "\n",
    "The TF value for a term in a document is determined using the following formula:\n",
    "\n",
    "\\[ \\text{TF}_{ij} = 1 + \\log_{10}(\\text{tf}_{ij} + 1) \\]\n",
    "\n",
    "- \\( \\text{TF}_{ij} \\): Term Frequency of term \\( j \\) in document \\( i \\)\n",
    "- \\( \\text{tf}_{ij} \\): Raw frequency of term \\( j \\) in document \\( i \\)\n",
    "\n",
    "## Explanation of Formula\n",
    "\n",
    "1. **Addition of 1**: \n",
    "   - Adding 1 to the raw term frequency ensures that terms with zero occurrences are assigned a non-zero TF value.\n",
    "\n",
    "2. **Logarithmic Transformation**:\n",
    "   - The raw term frequency is transformed using a logarithmic function (base 10).\n",
    "   - Logarithmic scaling compresses the range of term frequencies, reducing the influence of very high frequencies.\n",
    "\n",
    "3. **Normalization**:\n",
    "   - By adding 1 before taking the logarithm, the TF values are normalized, preventing extremely high frequencies from dominating the TF calculation.\n",
    "   - Normalization helps balance the importance of terms within documents, especially for documents with varying lengths.\n",
    "\n",
    "## Impact on Term Weight\n",
    "\n",
    "- **Increase in Weight**:\n",
    "  - The formula increases the weight assigned to terms based on their frequency in documents.\n",
    "  - Terms occurring more frequently in a document are assigned higher TF values, reflecting their relative importance within the document.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88a67f01-4112-4608-afe6-c75899eac51b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1.txt</th>\n",
       "      <th>2.txt</th>\n",
       "      <th>3.txt</th>\n",
       "      <th>7.txt</th>\n",
       "      <th>8.txt</th>\n",
       "      <th>9.txt</th>\n",
       "      <th>11.txt</th>\n",
       "      <th>12.txt</th>\n",
       "      <th>13.txt</th>\n",
       "      <th>14.txt</th>\n",
       "      <th>15.txt</th>\n",
       "      <th>16.txt</th>\n",
       "      <th>17.txt</th>\n",
       "      <th>18.txt</th>\n",
       "      <th>21.txt</th>\n",
       "      <th>22.txt</th>\n",
       "      <th>23.txt</th>\n",
       "      <th>24.txt</th>\n",
       "      <th>25.txt</th>\n",
       "      <th>26.txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71331005</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717–727</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71:2668-79</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71–101</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71–104</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71–80</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71–82</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71–83</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.954243</td>\n",
       "      <td>1.60206</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.477121</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.477121</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              1.txt    2.txt 3.txt     7.txt    8.txt 9.txt 11.txt   12.txt  \\\n",
       "71331005        1.0      1.0   1.0       1.0      1.0   1.0    1.0      1.0   \n",
       "717–727         1.0      1.0   1.0       1.0      1.0   1.0    1.0  1.30103   \n",
       "71:2668-79      1.0      1.0   1.0       1.0  1.30103   1.0    1.0      1.0   \n",
       "71–101          1.0      1.0   1.0   1.30103      1.0   1.0    1.0      1.0   \n",
       "71–104          1.0      1.0   1.0       1.0      1.0   1.0    1.0  1.30103   \n",
       "71–80           1.0      1.0   1.0       1.0      1.0   1.0    1.0      1.0   \n",
       "71–82           1.0      1.0   1.0       1.0      1.0   1.0    1.0  1.30103   \n",
       "71–83           1.0      1.0   1.0       1.0      1.0   1.0    1.0  1.30103   \n",
       "72              1.0  1.30103   1.0  1.954243  1.60206   1.0    1.0      1.0   \n",
       "720         1.30103      1.0   1.0       1.0      1.0   1.0    1.0      1.0   \n",
       "\n",
       "           13.txt 14.txt 15.txt    16.txt 17.txt 18.txt    21.txt 22.txt  \\\n",
       "71331005      1.0    1.0    1.0       1.0    1.0    1.0       1.0    1.0   \n",
       "717–727       1.0    1.0    1.0       1.0    1.0    1.0       1.0    1.0   \n",
       "71:2668-79    1.0    1.0    1.0       1.0    1.0    1.0       1.0    1.0   \n",
       "71–101        1.0    1.0    1.0       1.0    1.0    1.0       1.0    1.0   \n",
       "71–104        1.0    1.0    1.0       1.0    1.0    1.0       1.0    1.0   \n",
       "71–80         1.0    1.0    1.0   1.30103    1.0    1.0       1.0    1.0   \n",
       "71–82         1.0    1.0    1.0       1.0    1.0    1.0       1.0    1.0   \n",
       "71–83         1.0    1.0    1.0       1.0    1.0    1.0       1.0    1.0   \n",
       "72            1.0    1.0    1.0  1.477121    1.0    1.0       1.0    1.0   \n",
       "720           1.0    1.0    1.0       1.0    1.0    1.0  1.477121    1.0   \n",
       "\n",
       "           23.txt   24.txt 25.txt   26.txt  \n",
       "71331005      1.0  1.30103    1.0      1.0  \n",
       "717–727       1.0      1.0    1.0      1.0  \n",
       "71:2668-79    1.0      1.0    1.0      1.0  \n",
       "71–101        1.0      1.0    1.0      1.0  \n",
       "71–104        1.0      1.0    1.0      1.0  \n",
       "71–80         1.0      1.0    1.0      1.0  \n",
       "71–82         1.0      1.0    1.0      1.0  \n",
       "71–83         1.0      1.0    1.0      1.0  \n",
       "72            1.0  1.30103    1.0  1.30103  \n",
       "720           1.0      1.0    1.0      1.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.head()\n",
    "tf_matrix.iloc[2190:2200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2d1c294-62d5-4604-980a-cd9cb2f3ee8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1.txt</th>\n",
       "      <th>2.txt</th>\n",
       "      <th>3.txt</th>\n",
       "      <th>7.txt</th>\n",
       "      <th>8.txt</th>\n",
       "      <th>9.txt</th>\n",
       "      <th>11.txt</th>\n",
       "      <th>12.txt</th>\n",
       "      <th>13.txt</th>\n",
       "      <th>14.txt</th>\n",
       "      <th>15.txt</th>\n",
       "      <th>16.txt</th>\n",
       "      <th>17.txt</th>\n",
       "      <th>18.txt</th>\n",
       "      <th>21.txt</th>\n",
       "      <th>22.txt</th>\n",
       "      <th>23.txt</th>\n",
       "      <th>24.txt</th>\n",
       "      <th>25.txt</th>\n",
       "      <th>26.txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>miami</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>miao</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.60206</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.90309</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>miaoa</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>miaozhang</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mich</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>michael</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.778151</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.60206</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>michaelcollin</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>michailidi</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>michal</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.477121</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>michalski</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.477121</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              1.txt 2.txt    3.txt     7.txt 8.txt 9.txt 11.txt 12.txt  \\\n",
       "miami           1.0   1.0  1.30103   1.30103   1.0   1.0    1.0    1.0   \n",
       "miao            1.0   1.0      1.0       1.0   1.0   1.0    1.0    1.0   \n",
       "miaoa           1.0   1.0      1.0       1.0   1.0   1.0    1.0    1.0   \n",
       "miaozhang       1.0   1.0      1.0       1.0   1.0   1.0    1.0    1.0   \n",
       "mich            1.0   1.0      1.0       1.0   1.0   1.0    1.0    1.0   \n",
       "michael         1.0   1.0      1.0       1.0   1.0   1.0    1.0    1.0   \n",
       "michaelcollin   1.0   1.0      1.0       1.0   1.0   1.0    1.0    1.0   \n",
       "michailidi      1.0   1.0      1.0       1.0   1.0   1.0    1.0    1.0   \n",
       "michal          1.0   1.0      1.0       1.0   1.0   1.0    1.0    1.0   \n",
       "michalski       1.0   1.0      1.0  1.477121   1.0   1.0    1.0    1.0   \n",
       "\n",
       "                13.txt   14.txt   15.txt   16.txt    17.txt    18.txt  \\\n",
       "miami              1.0      1.0      1.0      1.0       1.0       1.0   \n",
       "miao               1.0      1.0      1.0  1.60206       1.0       1.0   \n",
       "miaoa              1.0      1.0      1.0      1.0       1.0       1.0   \n",
       "miaozhang          1.0      1.0      1.0      1.0       1.0       1.0   \n",
       "mich               1.0      1.0  1.30103      1.0       1.0       1.0   \n",
       "michael            1.0      1.0      1.0      1.0       1.0  1.778151   \n",
       "michaelcollin      1.0      1.0      1.0      1.0       1.0       1.0   \n",
       "michailidi     1.30103  1.30103      1.0      1.0       1.0       1.0   \n",
       "michal             1.0      1.0      1.0      1.0  1.477121       1.0   \n",
       "michalski          1.0      1.0      1.0      1.0       1.0       1.0   \n",
       "\n",
       "                21.txt   22.txt 23.txt   24.txt 25.txt 26.txt  \n",
       "miami              1.0      1.0    1.0      1.0    1.0    1.0  \n",
       "miao               1.0      1.0    1.0  1.90309    1.0    1.0  \n",
       "miaoa              1.0      1.0    1.0  1.30103    1.0    1.0  \n",
       "miaozhang          1.0  1.30103    1.0      1.0    1.0    1.0  \n",
       "mich               1.0      1.0    1.0      1.0    1.0    1.0  \n",
       "michael        1.30103  1.60206    1.0      1.0    1.0    1.0  \n",
       "michaelcollin      1.0  1.30103    1.0      1.0    1.0    1.0  \n",
       "michailidi         1.0      1.0    1.0      1.0    1.0    1.0  \n",
       "michal             1.0      1.0    1.0      1.0    1.0    1.0  \n",
       "michalski          1.0      1.0    1.0      1.0    1.0    1.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = tf_matrix.T\n",
    "tf_matrix.iloc[10000:10010]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d605e4fb-879f-4ebd-ae68-e5387601f6c9",
   "metadata": {},
   "source": [
    "# IDF: INVERSE DOCUMENT FREQUENCY\n",
    "\n",
    "# Inverse Document Frequency (IDF) Matrix\r\n",
    "\r\n",
    "This section describes the generation of the Inverse Document Frequency (IDF) matrix from the Bag-of-Words representation.\r\n",
    "\r\n",
    "## Construction of IDF Matrix\r\n",
    "\r\n",
    "An IDF matrix (`idf_matrix`) is constructed to represent the inverse document frequencies of terms.\r\n",
    "\r\n",
    "- **Matrix Structure**:\r\n",
    "  - The IDF matrix has a single row representing IDF values for all terms.\r\n",
    "  - Each column represents a unique stemmed term extracted from the documents.\r\n",
    "\r\n",
    "## Generating IDF Data\r\n",
    "\r\n",
    "The IDF matrix is populated with IDF values calculated based on document frequencies.\r\n",
    "\r\n",
    "- **IDF Calculation**:\r\n",
    "  - IDF values are calculated using the formula: \\( \\log_{10} \\left( \\frac{N}{df_t} \\right) \\), where \\( N \\) is the total number of documents and \\( df_t \\) is the number of documents containing term \\( t \\).\r\n",
    "\r\n",
    "- **IDF Calculation Process**:\r\n",
    "  - For each term in the document, the number of documents containing the term (\\( df_t \\)) is counted.\r\n",
    "  - IDF values are calculated based on the total number of documents and the document frequency of each term.\r\n",
    "  - The calculated IDF values are recorded in the corresponding cells of the IDF matrix.\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f196b6a-7c3b-45f1-a72c-7a3240a83bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_matrix = pd.DataFrame(columns=all_terms, index=['idf'])\n",
    "doc_fre = 0\n",
    "\n",
    "for term in all_terms:\n",
    "    for i in range(20):\n",
    "        if bow_df[term][i] > 0:\n",
    "            doc_fre += 1\n",
    "    id_fre = np.log10(len(doc) / (doc_fre)) \n",
    "    idf_matrix[term] = id_fre\n",
    "    doc_fre  = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74117c62-8e64-41e6-a1fa-c831ad2972bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>miami</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>miao</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>miaoa</th>\n",
       "      <td>1.301030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>miaozhang</th>\n",
       "      <td>1.301030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mich</th>\n",
       "      <td>1.301030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>michael</th>\n",
       "      <td>0.823909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>michaelcollin</th>\n",
       "      <td>1.301030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>michailidi</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>michal</th>\n",
       "      <td>1.301030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>michalski</th>\n",
       "      <td>1.301030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    idf\n",
       "miami          1.000000\n",
       "miao           1.000000\n",
       "miaoa          1.301030\n",
       "miaozhang      1.301030\n",
       "mich           1.301030\n",
       "michael        0.823909\n",
       "michaelcollin  1.301030\n",
       "michailidi     1.000000\n",
       "michal         1.301030\n",
       "michalski      1.301030"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf = idf_matrix.T\n",
    "idf.iloc[10000:10010]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9269ce18-9ee1-4821-a260-90dc9c6ab2ee",
   "metadata": {},
   "source": [
    "# COSINE SIMILARITY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c85162-a9c4-451d-987b-df4e620a8800",
   "metadata": {},
   "source": [
    "## WEIGHT MATRIX\n",
    "# Calculation of Weighted Term Frequencies\r\n",
    "\r\n",
    "This section describes the calculation of weighted term frequencies using TF-IDF values.\r\n",
    "\r\n",
    "## Construction of Weighted Matrix\r\n",
    "\r\n",
    "A weighted matrix (`weight`) is constructed to represent the weighted term frequencies based on TF-IDF values.\r\n",
    "\r\n",
    "- **Matrix Structure**:\r\n",
    "  - Each row represents a document.\r\n",
    "  - Each column represents a unique stemmed term extracted from the documents.\r\n",
    "\r\n",
    "## Generating Weighted Data\r\n",
    "\r\n",
    "The weighted matrix is populated with weighted term frequencies calculated using TF-IDF values.\r\n",
    "\r\n",
    "- **Weighted Calculation**:\r\n",
    "  - Weighted term frequencies are calculated by multiplying TF values with IDF values for each term in each document.\r\n",
    "\r\n",
    "- **Weighted Calculation Process**:\r\n",
    "  - For each term in each document, the TF value is multiplied by the corresponding IDF value to calculate the weighted term frequency.\r\n",
    "  - The calculated weighted term frequencies are recorded in the corresponding cells of the weighted matrix.\r\n",
    "\r\n",
    "## Cosine Similarity\r\n",
    "\r\n",
    "Once the weighted matrix is constructed, cosine similarity can be calculated to measure the similarity between documents.\r\n",
    "\r\n",
    "- **Cosine Similarity**:\r\n",
    "  - Cosine similarity measures the cosine of the angle between two vectors, representing document representations in a high-dimensional space.\r\n",
    "  - Higher cosine similarity values indicate greater similarity between documents, while values closer to 0 indicate dissimilarity.\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0418fb3-6b20-426b-9b7d-ee0a2aee526e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = pd.DataFrame(columns=all_terms, index=doc)\n",
    "\n",
    "for i in range(len(doc)):\n",
    "    for j in range(len(all_terms)):\n",
    "        weighted_value = tf.iloc[i, j] * idf.iloc[i, 0]\n",
    "        weight.iloc[i, j] = weighted_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2320a73-f563-4989-b51e-13d3f341f281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1.txt</th>\n",
       "      <th>2.txt</th>\n",
       "      <th>3.txt</th>\n",
       "      <th>7.txt</th>\n",
       "      <th>8.txt</th>\n",
       "      <th>9.txt</th>\n",
       "      <th>11.txt</th>\n",
       "      <th>12.txt</th>\n",
       "      <th>13.txt</th>\n",
       "      <th>14.txt</th>\n",
       "      <th>15.txt</th>\n",
       "      <th>16.txt</th>\n",
       "      <th>17.txt</th>\n",
       "      <th>18.txt</th>\n",
       "      <th>21.txt</th>\n",
       "      <th>22.txt</th>\n",
       "      <th>23.txt</th>\n",
       "      <th>24.txt</th>\n",
       "      <th>25.txt</th>\n",
       "      <th>26.txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>miami</th>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>miao</th>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>2.084328</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>2.475977</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>miaoa</th>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.692679</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>miaozhang</th>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.692679</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mich</th>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.692679</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>michael</th>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>2.313428</td>\n",
       "      <td>1.692679</td>\n",
       "      <td>2.084328</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>michaelcollin</th>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.692679</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>michailidi</th>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.692679</td>\n",
       "      <td>1.692679</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>michal</th>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.921779</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>michalski</th>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.477121</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 1.txt    2.txt    3.txt     7.txt    8.txt    9.txt   11.txt  \\\n",
       "miami          1.30103  1.30103  1.30103   1.30103  1.30103  1.30103  1.30103   \n",
       "miao           1.30103  1.30103      1.0       1.0  1.30103  1.30103  1.30103   \n",
       "miaoa          1.30103  1.30103      1.0       1.0  1.30103  1.30103  1.30103   \n",
       "miaozhang      1.30103  1.30103      1.0       1.0  1.30103  1.30103  1.30103   \n",
       "mich           1.30103  1.30103      1.0       1.0  1.30103  1.30103  1.30103   \n",
       "michael        1.30103  1.30103      1.0       1.0  1.30103  1.30103  1.30103   \n",
       "michaelcollin  1.30103  1.30103      1.0       1.0  1.30103  1.30103  1.30103   \n",
       "michailidi     1.30103  1.30103      1.0       1.0  1.30103  1.30103  1.30103   \n",
       "michal         1.30103  1.30103      1.0       1.0  1.30103  1.30103  1.30103   \n",
       "michalski      1.30103  1.30103      1.0  1.477121  1.30103  1.30103  1.30103   \n",
       "\n",
       "                12.txt    13.txt    14.txt    15.txt    16.txt    17.txt  \\\n",
       "miami          1.30103   1.30103   1.30103   1.30103   1.30103   1.30103   \n",
       "miao           1.30103   1.30103   1.30103   1.30103  2.084328   1.30103   \n",
       "miaoa          1.30103   1.30103   1.30103   1.30103   1.30103   1.30103   \n",
       "miaozhang      1.30103   1.30103   1.30103   1.30103   1.30103   1.30103   \n",
       "mich           1.30103   1.30103   1.30103  1.692679   1.30103   1.30103   \n",
       "michael        1.30103   1.30103   1.30103   1.30103   1.30103   1.30103   \n",
       "michaelcollin  1.30103   1.30103   1.30103   1.30103   1.30103   1.30103   \n",
       "michailidi     1.30103  1.692679  1.692679   1.30103   1.30103   1.30103   \n",
       "michal         1.30103   1.30103   1.30103   1.30103   1.30103  1.921779   \n",
       "michalski      1.30103   1.30103   1.30103   1.30103   1.30103   1.30103   \n",
       "\n",
       "                 18.txt    21.txt    22.txt   23.txt    24.txt   25.txt  \\\n",
       "miami           1.30103   1.30103   1.30103  1.30103   1.30103  1.30103   \n",
       "miao            1.30103   1.30103   1.30103  1.30103  2.475977  1.30103   \n",
       "miaoa           1.30103   1.30103   1.30103  1.30103  1.692679  1.30103   \n",
       "miaozhang       1.30103   1.30103  1.692679  1.30103   1.30103  1.30103   \n",
       "mich            1.30103   1.30103   1.30103  1.30103   1.30103  1.30103   \n",
       "michael        2.313428  1.692679  2.084328  1.30103   1.30103  1.30103   \n",
       "michaelcollin   1.30103   1.30103  1.692679  1.30103   1.30103  1.30103   \n",
       "michailidi      1.30103   1.30103   1.30103  1.30103   1.30103  1.30103   \n",
       "michal          1.30103   1.30103   1.30103  1.30103   1.30103  1.30103   \n",
       "michalski       1.30103   1.30103   1.30103  1.30103   1.30103  1.30103   \n",
       "\n",
       "                26.txt  \n",
       "miami          1.30103  \n",
       "miao           1.30103  \n",
       "miaoa          1.30103  \n",
       "miaozhang      1.30103  \n",
       "mich           1.30103  \n",
       "michael        1.30103  \n",
       "michaelcollin  1.30103  \n",
       "michailidi     1.30103  \n",
       "michal         1.30103  \n",
       "michalski      1.30103  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_matrix = weight.T\n",
    "weight_matrix[10000:10010]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1671d6-ba56-4cf6-9d71-501b9e9f4631",
   "metadata": {},
   "source": [
    "## NORMALIZING\n",
    "# Normalization of Weighted Term Frequencies\r\n",
    "\r\n",
    "This section describes the normalization of weighted term frequencies in the weighted matrix.\r\n",
    "\r\n",
    "## Normalization Process\r\n",
    "\r\n",
    "The weighted term frequencies in the weighted matrix are normalized to ensure consistent representation across documents.\r\n",
    "\r\n",
    "- **Normalization Formula**:\r\n",
    "  - Each term frequency in the weighted matrix is divided by the square root of the sum of squares of all term frequencies in the corresponding column.\r\n",
    "\r\n",
    "- **Normalization Steps**:\r\n",
    "  1. Calculate the sum of squares of term frequencies for each term across all documents.\r\n",
    "  2. Compute the square root of the sum of squares to obtain the normalization factor.\r\n",
    "  3. Normalize each term frequency by dividing it by the normalization factor.\r\n",
    "\r\n",
    "- **Normalization Result**:\r\n",
    "  - After normalization, each term frequency represents its relative importance within the document and facilitates comparison between documents.\r\n",
    "\r\n",
    "## Application of Normalization\r\n",
    "\r\n",
    "Normalized weighted term frequencies enable more accurate comparison and analysis of document similarity.\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20847605-dd2f-49ab-851c-764837848fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_values = []\n",
    "for i in range(20):\n",
    "    sum_of_squares = np.sum(weight_matrix.iloc[:, i] ** 2)\n",
    "    normalized_values.append(np.sqrt(sum_of_squares))\n",
    "\n",
    "for i in range(20):\n",
    "    for j in range(len(all_terms)):\n",
    "        weight_matrix.iloc[j, i] = weight_matrix.iloc[j, i] / normalized_values[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97ae1f7e-75e9-4891-9f47-15af21e62125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[183.6902036050944,\n",
       " 180.24936575267787,\n",
       " 133.65144284829657,\n",
       " 154.6767650174217,\n",
       " 177.116707498794,\n",
       " 173.54479017716977,\n",
       " 170.6589589728953,\n",
       " 189.368560030639,\n",
       " 184.6295851150998,\n",
       " 184.6295851150998,\n",
       " 179.8988760705069,\n",
       " 183.61205045211517,\n",
       " 179.78112153013782,\n",
       " 175.50482003283742,\n",
       " 178.47844728591338,\n",
       " 190.28086244780235,\n",
       " 172.2767334312915,\n",
       " 174.37966612878904,\n",
       " 174.639548811969,\n",
       " 179.46324612046334]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(normalized_values)\n",
    "normalized_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513f7dcd-b020-4a8a-96d6-0d246292d6ad",
   "metadata": {},
   "source": [
    "# SAVE INTO CSV FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ddc4b675-6844-49a9-8c3f-a86ab1b0910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'vector_space_model.csv'\n",
    "weight_matrix.to_csv(csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08653ba1-2222-4ea4-8b5b-30672d182993",
   "metadata": {},
   "source": [
    "# QUERY PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3adcac29-599b-4c60-9eee-032159687fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    score = []\n",
    "    q_term = []\n",
    "    q_vec = {}\n",
    "    doc_vec = {}\n",
    "    result = {}\n",
    "    magnitude = 0\n",
    "    sum = 0\n",
    "    \n",
    "    q_token = nltk.word_tokenize(query)\n",
    "    for x in q_token:\n",
    "        stemmed_x = porter.stem(x)\n",
    "        q_term.append(stemmed_x)\n",
    "\n",
    "    for x in q_term:\n",
    "        q_vec[x] = idf.loc[x][0]\n",
    "\n",
    "    for x,y in q_vec.items():\n",
    "        magnitude = magnitude + y**2\n",
    "\n",
    "    for x,y in q_vec.items():\n",
    "        q_vec[x] = y/np.sqrt(magnitude)\n",
    "\n",
    "    for x,y in q_vec.items():\n",
    "        doc_vec[x] = []\n",
    "        for i in range(20):\n",
    "            doc_vec[x].append(weight_matrix.loc[x][i])\n",
    "    \n",
    "    for i in range(20):\n",
    "        for x,y in doc_vec.items():\n",
    "            sum = sum + y[i]*(q_vec[x])\n",
    "        score.append((sum,doc[i]))\n",
    "    score.sort()\n",
    "\n",
    "    for i in range(20):\n",
    "        if score[i][0] < 0.05:\n",
    "            # print(score[i])\n",
    "            result[score[i][1]] = score[i][0]\n",
    "    return result,score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bdd315-8931-4e63-bf0f-6aa144da5bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR EXIT ENTER EMPTY STRING\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Search:  machine learning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Rank\n",
      "1.txt  0.027669\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Search:  books\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Rank\n",
      "1.txt  0.007083\n",
      "2.txt  0.014301\n",
      "3.txt  0.021783\n",
      "7.txt  0.030194\n",
      "8.txt  0.037540\n",
      "9.txt  0.045037\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = True\n",
    "print('FOR EXIT ENTER EMPTY STRING')\n",
    "while start:\n",
    "    query = input('Search: ')\n",
    "    if query != \"\":\n",
    "        result, score = search(query)\n",
    "        res_df = pd.DataFrame.from_dict(result, orient='index', columns=['Rank'])\n",
    "        print(res_df)\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        start = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b99a137-1e6c-4c87-bf4f-aad5a9cb2ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
